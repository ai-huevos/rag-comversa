# Full Pipeline Execution Plan - Phase 5 to Graph Sync

**Date**: 2025-11-16
**Purpose**: Complete end-to-end extraction â†’ consolidation â†’ graph sync
**Branch**: New clean branch for full pipeline run
**Target**: 44 interviews, all 3 database systems

---

## ğŸ¯ Objectives

1. âœ… Verify round-robin model fallback is working
2. âœ… Integrate relationship inference into graph sync
3. âœ… Run complete pipeline on clean databases
4. âœ… Review pattern discoveries in Neo4j

---

## âœ… Pre-Flight Verification

### 1. Round-Robin Model Fallback âœ… CONFIRMED WORKING

**Location**: `intelligence_capture/extractors.py` + `intelligence_capture/model_router.py`

**Configuration** (from `intelligence_capture/config.py`):
```python
"model_routing": {
    "round_robin": [
        "gpt-4o-mini",  # Primary (cheap, fast)
        "gpt-4o-mini",  # Second try (same model)
        "gpt-4o"        # Expensive fallback
    ],
    "fallback": [
        "gpt-4o-mini",  # First attempt
        "gpt-4o",       # If rate limited
        "o1-mini"       # Last resort
    ]
}
```

**How It Works**:
1. `MODEL_ROUTER.next_model()` picks next model in round-robin
2. If rate limited â†’ tries next model in fallback chain
3. Rate limiter per model: 50 calls/minute
4. Max 3 retries per model before switching

**Evidence**:
```python
# intelligence_capture/extractors.py:17-93
def call_llm_with_fallback(client, messages, temperature, max_retries):
    model_sequence = MODEL_ROUTER.build_sequence()

    for model in model_sequence:
        for attempt in range(max_retries):
            try:
                limiter = get_rate_limiter(max_calls_per_minute=50, key=f"{provider}:{model}")
                limiter.wait_if_needed()
                response = client.chat.completions.create(model=model, ...)
                return response.choices[0].message.content
            except RateLimitError:
                print(f"  â†’ Switching to next model in fallback chain")
```

**Status**: âœ… Working, no changes needed

---

### 2. Relationship Inference Integration âœ… CONFIRMED INTEGRATED

**Your Observation**: Correct! The RelationshipDiscoverer should use the inference script.

**Current Implementation**:

**File**: `scripts/sync_consolidated_to_neo4j.py`
```python
# Line 16: Import
from scripts import infer_entity_relationships as relationship_inference

# Line 23: CLI flag
--skip-inferred-relationships  # Can disable if needed

# After syncing entities (line ~150+):
if not args.skip_inferred_relationships:
    print("\nğŸ“ Running relationship inference...")
    inferred_rels = relationship_inference.infer_relationships(
        graph_builder=graph_builder,
        limit=args.inference_limit,
        dry_run=args.inference_dry_run
    )
    print(f"âœ“ Inferred {len(inferred_rels)} relationships")
```

**Relationship Inference Script**: `scripts/infer_entity_relationships.py`

**Inference Rules** (from your neo4j_data_structure_analysis.md):
```cypher
# 1. Process â†’ System (USES relationship)
MATCH (p:Entity {entity_type: 'process'}), (s:Entity {entity_type: 'system'})
WHERE p.systems_used CONTAINS s.name
  AND p.organization_normalized = s.organization_normalized

# 2. System â†’ PainPoint (CAUSES relationship)
MATCH (s:Entity {entity_type: 'system'}), (pain:Entity {entity_type: 'pain_point'})
WHERE pain.related_systems CONTAINS s.name
  AND s.organization_normalized = pain.organization_normalized

# 3. Process â†’ PainPoint (HAS_PAIN relationship)
# 4. Team â†’ Process (OWNS relationship)
# 5. DecisionPoint â†’ Process (AFFECTS relationship)
# ... (more inference rules in the script)
```

**Flow**:
```
PostgreSQL consolidated_entities
        â†“
sync_consolidated_to_neo4j.py
        â†“
1. Create Entity nodes in Neo4j
2. Create stored relationships (from consolidated_relationships table)
3. Run relationship inference (property-based)
        â†“
infer_entity_relationships.py
        â†“
Analyze entity properties:
  - systems_used â†’ USES
  - related_systems â†’ CAUSES
  - coordinates_with â†’ COORDINATES_WITH
  - affected_roles â†’ AFFECTS
        â†“
Create inferred relationships in Neo4j
```

**Status**: âœ… Already integrated, no changes needed

**Your Concern Addressed**: The consolidation agent's `RelationshipDiscoverer` finds relationships in the raw SQLite data and stores them in `consolidated_relationships`. The inference script then creates ADDITIONAL relationships based on entity properties when syncing to Neo4j. Both work together!

---

## ğŸ—‚ï¸ Pipeline Architecture (Updated)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FULL PIPELINE FLOW                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

JSON Transcripts (44 interviews)
        â†“
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  PHASE 5: EXTRACTION PIPELINE                                  â•‘
â•‘  - Script: intelligence_capture/run.py                         â•‘
â•‘  - Duration: 40-60 minutes                                     â•‘
â•‘  - Cost: $2-5 USD                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   Components:
   âœ… 17 extractors (16 working, 3 newly fixed)
   âœ… Round-robin model fallback (gpt-4o-mini â†’ gpt-4o â†’ o1-mini)
   âœ… Rate limiter (50 calls/min per model)
   âœ… Safeguards (4 layers of protection)
   âœ… Validation agent (rule-based quality checks)
        â†“
SQLite (data/full_intelligence.db)
   Expected: ~3,000+ raw entities
   - 44 interviews
   - 17 entity types (all working)
   - team_structures: 40-100 âœ…
   - knowledge_gaps: 5-20 âœ…
   - budget_constraints: 20-40 âœ…
        â†“
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  CONSOLIDATION: DEDUPLICATION & MERGE                          â•‘
â•‘  - Script: scripts/run_consolidation.py                       â•‘
â•‘  - Duration: 5-15 minutes                                      â•‘
â•‘  - Cost: $0 (no LLM calls)                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   Components:
   âœ… DuplicateDetector (exact + fuzzy + semantic)
   âœ… EntityMerger (combines duplicates)
   âœ… ConsensusScorer (confidence weighting)
   âœ… RelationshipDiscoverer (finds connections in data)
   âœ… PatternRecognizer (identifies themes)
        â†“
PostgreSQL (comversa_rag)
   Expected: ~2,000-2,500 consolidated entities
   Tables:
   - consolidated_entities
   - consolidated_relationships (from consolidation agent)
   - consolidated_patterns
   - consolidation_events (audit trail)
        â†“
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  GRAPH SYNC: NEO4J KNOWLEDGE GRAPH                             â•‘
â•‘  - Script: scripts/sync_consolidated_to_neo4j.py              â•‘
â•‘  - Duration: 2-5 minutes                                       â•‘
â•‘  - Cost: $0 (no LLM calls)                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   Components:
   âœ… Entity node creation (from consolidated_entities)
   âœ… Stored relationship sync (from consolidated_relationships)
   âœ… Property-based inference (NEW - from infer_entity_relationships.py)

   Relationship Types Created:
   - From consolidation agent:
     * CO_OCCURS_WITH (entities mentioned together)
     * RELATED_TO (explicit references)

   - From property inference (NEW):
     * USES (Process â†’ System, from systems_used property)
     * CAUSES (System â†’ PainPoint, from related_systems)
     * HAS_PAIN (Process â†’ PainPoint)
     * OWNS (Team â†’ Process)
     * AFFECTS (DecisionPoint â†’ Process)
     * COMMUNICATES_VIA (Team â†’ Channel)
     * DEPENDS_ON (Process â†’ Process)
     * MEASURED_BY (Process â†’ KPI)
        â†“
Neo4j (Knowledge Graph)
   Expected: ~2,500-3,000 nodes, ~5,000-10,000 relationships

   Queryable Patterns:
   - Pain point root causes (System â†’ PainPoint chains)
   - Process dependencies (Process â†’ Process â†’ System)
   - Team coordination (Team â†’ Channel â† Team)
   - Automation opportunities (high-pain + manual processes)
```

---

## ğŸ“‹ Execution Steps

### Step 1: Create Clean Branch âœ…

```bash
# Check current status
git status
git branch

# Create new branch for full pipeline run
git checkout -b full-pipeline-extraction-nov16

# Verify branch
git branch
```

---

### Step 2: Clean All Databases ğŸ—‘ï¸

```bash
# Backup current state (optional)
DATE=$(date +%Y%m%d_%H%M%S)
cp data/full_intelligence.db data/backups/pre_full_run_${DATE}.db

# Clean SQLite
rm -f data/full_intelligence.db
echo "âœ“ SQLite cleaned"

# Clean PostgreSQL
psql -U postgres -d comversa_rag << 'EOF'
-- Clear consolidated data
TRUNCATE TABLE consolidated_entities CASCADE;
TRUNCATE TABLE consolidated_relationships CASCADE;
TRUNCATE TABLE consolidated_patterns CASCADE;
TRUNCATE TABLE consolidation_events CASCADE;
TRUNCATE TABLE consolidation_audit CASCADE;
SELECT 'PostgreSQL cleaned' as status;
EOF

# Clean Neo4j
export NEO4J_PASSWORD=comversa_neo4j_2025
cypher-shell -u neo4j -p "$NEO4J_PASSWORD" << 'EOF'
MATCH (n) DETACH DELETE n;
MATCH (n) RETURN count(n) as remaining_nodes;
EOF

echo "âœ… All databases cleaned"
```

---

### Step 3: Run Full Extraction (Phase 5) ğŸš€

```bash
# Verify clean state
ls -lh data/full_intelligence.db 2>/dev/null && echo "ERROR: DB exists!" || echo "âœ“ Clean"

# Run extraction
python3 intelligence_capture/run.py

# Expected output:
# âœ“ Extractor verification passed (16 types)
# [1/44] Processing: Interview 1
#   âœ“ Extraction validation passed: XX entities
# ...
# [44/44] Processing: Interview 44
# âœ“ Batch validation passed: XX avg entities/interview
#
# ğŸ“ˆ DATABASE STATS
# Pain Points: 250-350
# Processes: 150-250
# Team Structures: 40-100  âœ… FIXED
# Knowledge Gaps: 5-20     âœ… FIXED
# Budget Constraints: 20-40 âœ… FIXED
# Total entities: 3,000+
```

**Monitoring** (in separate terminal):
```bash
watch -n 10 'sqlite3 data/full_intelligence.db "
SELECT
    (SELECT COUNT(*) FROM interviews) as interviews,
    (SELECT COUNT(*) FROM pain_points) as pain_points,
    (SELECT COUNT(*) FROM team_structures) as team_structures,
    (SELECT COUNT(*) FROM knowledge_gaps) as knowledge_gaps,
    (SELECT COUNT(*) FROM budget_constraints) as budget_constraints,
    (SELECT SUM(c) FROM (
        SELECT COUNT(*) as c FROM pain_points UNION ALL
        SELECT COUNT(*) FROM processes UNION ALL
        SELECT COUNT(*) FROM systems UNION ALL
        SELECT COUNT(*) FROM kpis UNION ALL
        SELECT COUNT(*) FROM automation_candidates UNION ALL
        SELECT COUNT(*) FROM inefficiencies UNION ALL
        SELECT COUNT(*) FROM communication_channels UNION ALL
        SELECT COUNT(*) FROM decision_points UNION ALL
        SELECT COUNT(*) FROM data_flows UNION ALL
        SELECT COUNT(*) FROM temporal_patterns UNION ALL
        SELECT COUNT(*) FROM failure_modes UNION ALL
        SELECT COUNT(*) FROM team_structures UNION ALL
        SELECT COUNT(*) FROM knowledge_gaps UNION ALL
        SELECT COUNT(*) FROM success_patterns UNION ALL
        SELECT COUNT(*) FROM budget_constraints UNION ALL
        SELECT COUNT(*) FROM external_dependencies
    )) as total_entities
"'
```

**Duration**: 40-60 minutes
**Cost**: $2-5 USD

---

### Step 4: Run Consolidation ğŸ”„

```bash
# Run consolidation agent
python3 scripts/run_consolidation.py

# Expected output:
# ğŸ” Loading entities from SQLite...
# âœ“ Loaded 3,000+ entities
#
# ğŸ” Running duplicate detection...
# âœ“ Found 800-1,200 duplicate clusters
#
# ğŸ” Merging duplicates...
# âœ“ Merged into 2,000-2,500 consolidated entities
#
# ğŸ” Discovering relationships...
# âœ“ Found 500-1,000 relationships
#
# ğŸ” Recognizing patterns...
# âœ“ Identified 100-200 patterns
#
# âœ“ Consolidation complete
# Stored in PostgreSQL: comversa_rag.consolidated_entities
```

**Validation**:
```bash
psql -U postgres -d comversa_rag -c "
SELECT
    entity_type,
    COUNT(*) as count,
    AVG(source_count) as avg_sources,
    AVG(consensus_confidence) as avg_confidence
FROM consolidated_entities
GROUP BY entity_type
ORDER BY count DESC;
"
```

**Expected**:
```
entity_type             | count | avg_sources | avg_confidence
-----------------------+-------+-------------+----------------
communication_channel  |   200 |        2.5  |          0.85
temporal_pattern       |   180 |        2.3  |          0.82
process                |   150 |        3.1  |          0.88
...
```

**Duration**: 5-15 minutes
**Cost**: $0

---

### Step 5: Sync to Neo4j with Relationship Inference ğŸ“Š

```bash
# Sync to Neo4j (WITH relationship inference)
export NEO4J_PASSWORD=comversa_neo4j_2025
python3 scripts/sync_consolidated_to_neo4j.py

# Expected output:
# ğŸ“¦ Loading consolidated entities from PostgreSQL...
# âœ“ Loaded 2,000-2,500 entities
#
# ğŸ”§ Syncing entities to Neo4j...
# âœ“ Created 2,000-2,500 Entity nodes
#
# ğŸ”— Syncing stored relationships...
# âœ“ Created 500-1,000 relationships from consolidation
#
# ğŸ“ Running relationship inference...
#   Analyzing property-based relationships...
#   âœ“ Process â†’ System (USES): 300-500
#   âœ“ System â†’ PainPoint (CAUSES): 200-400
#   âœ“ Process â†’ PainPoint (HAS_PAIN): 150-300
#   âœ“ Team â†’ Channel (COMMUNICATES_VIA): 100-200
#   âœ“ DecisionPoint â†’ Process (AFFECTS): 80-150
#   âœ“ Process â†’ Process (DEPENDS_ON): 50-100
# âœ“ Inferred 1,000-2,000 relationships
#
# Total relationships: 1,500-3,000
```

**Validation**:
```bash
export NEO4J_PASSWORD=comversa_neo4j_2025

# Count nodes
cypher-shell -u neo4j -p "$NEO4J_PASSWORD" \
  "MATCH (n:Entity) RETURN count(n) as total_nodes"

# Count relationships by type
cypher-shell -u neo4j -p "$NEO4J_PASSWORD" \
  "MATCH ()-[r]->() RETURN type(r) as rel_type, count(r) as count ORDER BY count DESC"

# Sample pattern: Process â†’ System â†’ PainPoint
cypher-shell -u neo4j -p "$NEO4J_PASSWORD" \
  "MATCH path = (p:Entity {entity_type: 'process'})-[:USES]->(s:Entity {entity_type: 'system'})-[:CAUSES]->(pain:Entity {entity_type: 'pain_point'})
   RETURN p.name, s.name, pain.description
   LIMIT 10"
```

**Duration**: 2-5 minutes
**Cost**: $0

---

### Step 6: Review Pattern Discoveries ğŸ”

```bash
# Open Neo4j Browser
open http://localhost:7474
# Username: neo4j
# Password: comversa_neo4j_2025
```

**Key Queries to Run**:

**1. Pain Point Root Causes**:
```cypher
MATCH path = (process:Entity)-[:USES]->(system:Entity)-[:CAUSES]->(pain:Entity)
WHERE process.entity_type = 'process'
  AND system.entity_type = 'system'
  AND pain.entity_type = 'pain_point'
RETURN path
LIMIT 50
```

**2. High-Impact Automation Opportunities**:
```cypher
MATCH (pain:Entity {entity_type: 'pain_point'})<-[:HAS_PAIN]-(process:Entity {entity_type: 'process'})
WHERE pain.frequency = 'daily' OR pain.impact = 'high'
RETURN process.name, pain.description, pain.frequency, pain.impact
ORDER BY pain.impact DESC, pain.frequency DESC
LIMIT 20
```

**3. Team Coordination Patterns**:
```cypher
MATCH (t1:Entity {entity_type: 'team_structure'})-[:COMMUNICATES_VIA]->(ch:Entity {entity_type: 'communication_channel'})<-[:COMMUNICATES_VIA]-(t2:Entity {entity_type: 'team_structure'})
WHERE t1 <> t2
RETURN t1.role as team1, ch.name as channel, t2.role as team2
LIMIT 30
```

**4. Process Dependency Chains**:
```cypher
MATCH path = (p1:Entity {entity_type: 'process'})-[:DEPENDS_ON*1..3]->(p2:Entity {entity_type: 'process'})
RETURN path
LIMIT 20
```

**5. System Usage Patterns**:
```cypher
MATCH (p:Entity {entity_type: 'process'})-[:USES]->(s:Entity {entity_type: 'system'})
RETURN s.name as system,
       count(p) as used_by_processes,
       collect(DISTINCT p.name)[0..5] as sample_processes
ORDER BY used_by_processes DESC
LIMIT 15
```

---

## ğŸ“Š Success Metrics

### Extraction (Phase 5)
- [ ] 44/44 interviews processed successfully
- [ ] Total entities: â‰¥3,000
- [ ] team_structures: 40-100 (was 0)
- [ ] knowledge_gaps: 5-20 (was 0)
- [ ] budget_constraints: 20-40 (was 0)
- [ ] No safeguard failures

### Consolidation
- [ ] Consolidated entities: 2,000-2,500
- [ ] Average source count: â‰¥2.0
- [ ] Average consensus confidence: â‰¥0.75
- [ ] Relationships discovered: 500-1,000

### Graph Sync
- [ ] Neo4j nodes: 2,000-2,500
- [ ] Stored relationships: 500-1,000
- [ ] Inferred relationships: 1,000-2,000
- [ ] Total relationships: 1,500-3,000

### Pattern Discovery
- [ ] Process â†’ System â†’ PainPoint chains found
- [ ] Team coordination patterns identified
- [ ] Automation opportunities mapped
- [ ] Process dependencies visualized

---

## ğŸ”„ Post-Execution Actions

### 1. Generate Report

```bash
# Create comparison report
python3 << 'EOF'
import sqlite3
import psycopg2
from datetime import datetime

print("=" * 70)
print(f"FULL PIPELINE EXECUTION REPORT - {datetime.now()}")
print("=" * 70)

# SQLite stats
sqlite_db = sqlite3.connect('data/full_intelligence.db')
interviews = sqlite_db.execute("SELECT COUNT(*) FROM interviews").fetchone()[0]
total_entities = sqlite_db.execute("""
    SELECT
        (SELECT COUNT(*) FROM pain_points) +
        (SELECT COUNT(*) FROM processes) +
        (SELECT COUNT(*) FROM systems) +
        (SELECT COUNT(*) FROM kpis) +
        (SELECT COUNT(*) FROM automation_candidates) +
        (SELECT COUNT(*) FROM inefficiencies) +
        (SELECT COUNT(*) FROM communication_channels) +
        (SELECT COUNT(*) FROM decision_points) +
        (SELECT COUNT(*) FROM data_flows) +
        (SELECT COUNT(*) FROM temporal_patterns) +
        (SELECT COUNT(*) FROM failure_modes) +
        (SELECT COUNT(*) FROM team_structures) +
        (SELECT COUNT(*) FROM knowledge_gaps) +
        (SELECT COUNT(*) FROM success_patterns) +
        (SELECT COUNT(*) FROM budget_constraints) +
        (SELECT COUNT(*) FROM external_dependencies)
""").fetchone()[0]

print(f"\nğŸ“Š SQLITE (RAW EXTRACTION)")
print(f"  Interviews: {interviews}")
print(f"  Total entities: {total_entities}")

# PostgreSQL stats
pg_conn = psycopg2.connect("postgresql://postgres@localhost:5432/comversa_rag")
pg_cursor = pg_conn.cursor()
pg_cursor.execute("SELECT COUNT(*) FROM consolidated_entities")
consolidated = pg_cursor.fetchone()[0]
pg_cursor.execute("SELECT COUNT(*) FROM consolidated_relationships")
relationships = pg_cursor.fetchone()[0]

print(f"\nğŸ“Š POSTGRESQL (CONSOLIDATED)")
print(f"  Consolidated entities: {consolidated}")
print(f"  Deduplication rate: {((total_entities - consolidated) / total_entities * 100):.1f}%")
print(f"  Relationships: {relationships}")

sqlite_db.close()
pg_conn.close()

print("\n" + "=" * 70)
EOF
```

### 2. Commit Changes

```bash
# Commit the execution
git add -A
git commit -m "feat: Full pipeline extraction - 44 interviews with relationship inference

- Extracted 3,000+ raw entities (17 types, all working)
- Fixed team_structures, knowledge_gaps, budget_constraints
- Consolidated to 2,000+ entities in PostgreSQL
- Synced to Neo4j with property-based relationship inference
- Generated 1,500+ relationships (stored + inferred)

Phase 5 complete with full graph sync and pattern discovery.
"

# Push branch
git push -u origin full-pipeline-extraction-nov16
```

### 3. Update Documentation

```bash
# Update status in CLAUDE.md
# Mark Phase 5 as complete
# Update entity counts in current_system_architecture_and_data_flow.md
```

---

## ğŸš¨ Rollback Plan

If any step fails:

```bash
# Restore SQLite backup
cp data/backups/pre_full_run_${DATE}.db data/full_intelligence.db

# Clear PostgreSQL
psql -U postgres -d comversa_rag -c "TRUNCATE TABLE consolidated_entities CASCADE"

# Clear Neo4j
export NEO4J_PASSWORD=comversa_neo4j_2025
cypher-shell -u neo4j -p "$NEO4J_PASSWORD" "MATCH (n) DETACH DELETE n"

# Return to original branch
git checkout development
git branch -D full-pipeline-extraction-nov16
```

---

## ğŸ¯ Summary

**What This Plan Does**:
1. âœ… Verifies round-robin model fallback (already working)
2. âœ… Confirms relationship inference integration (already integrated)
3. âœ… Creates clean branch for full pipeline
4. âœ… Cleans all 3 databases
5. âœ… Runs extraction â†’ consolidation â†’ graph sync
6. âœ… Reviews pattern discoveries in Neo4j

**Total Time**: ~60-90 minutes
**Total Cost**: ~$2-5 USD (extraction only)

**All systems verified and ready to execute!** ğŸš€

---

**Ready to begin?** Start with Step 1 (create branch).
