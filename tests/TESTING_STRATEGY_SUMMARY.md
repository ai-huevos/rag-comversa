# Estrategia Completa de Testing - Agente RAG (Task 10)

**Pregunta**: "Si el agente corre con mocks, Â¿cÃ³mo sabemos que funciona?"

**Respuesta**: Usamos una estrategia de testing en 3 niveles que combina velocidad, confianza y validaciÃ³n real.

---

## La PirÃ¡mide de Testing

```
                        Confianza
                           â†‘
                  /\       |
                 /  \      |    Manual Tests
                /____\     |    (~20 min, $0.02-0.05)
               /      \    |    âœ… 100% confianza real
              /________\   |    âŒ Lento, manual
             /          \  |
            /   Unit     \ |    Integration Tests
           /    Tests     \|    (~5 min, $0.005)
          /______________  \    âœ… Valida conexiones reales
         /                  \   âš ï¸ Requiere setup
        /____________________\
                               Unit Tests
                               (~2 sec, gratis)
                               âœ… RÃ¡pido, automatizable
                               âš ï¸ No valida integraciÃ³n

                               â† Velocidad
```

---

## Nivel 1: Unit Tests con Mocks (85+ tests)

### Â¿QuÃ© Prueban?

- âœ… **LÃ³gica del agente**: Decisiones, flujos, algoritmos
- âœ… **Manejo de errores**: Fallback, recovery, edge cases
- âœ… **GestiÃ³n de estado**: Sesiones, contexto, cachÃ©
- âœ… **Interfaces**: Contratos entre componentes

### Â¿QuÃ© NO Prueban?

- âŒ Que PostgreSQL estÃ© corriendo
- âŒ Que Neo4j tenga datos correctos
- âŒ Que OpenAI API responda
- âŒ Que las queries SQL sean vÃ¡lidas
- âŒ Que los Ã­ndices estÃ©n optimizados

### CuÃ¡ndo Usar

```bash
# Durante desarrollo (cada 5 minutos)
pytest tests/test_agent_session.py -q

# Antes de commit
pytest tests/test_agent_*.py -v

# En CI/CD (siempre)
pytest tests/test_agent_*.py --cov=agent
```

### Ejemplo

```python
def test_query_execution_success(self, rag_agent):
    """Debe ejecutar query exitosamente"""
    # Mock del resultado
    mock_result = MagicMock()
    mock_result.data = "Los sistemas principales son X, Y, Z."

    rag_agent.agent.run = AsyncMock(return_value=mock_result)

    # Ejecutar query
    response = asyncio.run(
        rag_agent.query(
            query="Â¿QuÃ© sistemas hay?",
            org_id="los_tajibos",
        )
    )

    # âœ… Prueba que la LÃ“GICA funciona
    # âŒ NO prueba que PostgreSQL funciona
    assert "answer" in response
    assert response["model"] == "gpt-4o-mini"
```

**Valor**: Feedback instantÃ¡neo sobre lÃ³gica del cÃ³digo.

---

## Nivel 2: Integration Tests con Deps Reales (10+ tests)

### Â¿QuÃ© Prueban?

- âœ… **Conexiones reales**: PostgreSQL + Neo4j + OpenAI conectan
- âœ… **Queries funcionan**: SQL y Cypher se ejecutan correctamente
- âœ… **Datos persisten**: Sesiones y telemetrÃ­a se guardan
- âœ… **Costos reales**: ValidaciÃ³n de gasto por query
- âœ… **Latencia real**: Performance con datos reales

### Â¿QuÃ© NO Prueban?

- âŒ ExploraciÃ³n manual de edge cases
- âŒ UX de conversaciones largas
- âŒ Calidad de respuestas (subjetivo)
- âŒ Comportamiento bajo carga extrema

### CuÃ¡ndo Usar

```bash
# DespuÃ©s de cambios en BD
pytest tests/test_agent_real_integration.py -v -m real_integration

# Antes de PR (si cambiÃ³ conexiones)
pytest tests/test_agent_real_integration.py -v -m real_integration

# Antes de deploy a staging
pytest tests/test_agent_real_integration.py -v -m real_integration
```

### Ejemplo

```python
@pytest.mark.asyncio
async def test_database_connection_works(self, real_agent):
    """Debe conectarse a PostgreSQL exitosamente"""
    # âœ… ConexiÃ³n REAL a PostgreSQL
    async with real_agent.db_pool.acquire() as conn:
        result = await conn.fetchval("SELECT 1")
        assert result == 1

    # âœ… Prueba que PostgreSQL estÃ¡ corriendo
    # âœ… Prueba que las credenciales son correctas
    # âœ… Prueba que pgvector estÃ¡ instalado
```

**Costo**: ~$0.005 (0.5 centavos) por ejecuciÃ³n completa.

**Valor**: Confianza de que las conexiones reales funcionan.

---

## Nivel 3: Manual Tests (Checklist de 20 min)

### Â¿QuÃ© Prueban?

- âœ… **Flujos completos de usuario**: Conversaciones naturales
- âœ… **Edge cases**: Casos que los tests automÃ¡ticos no cubren
- âœ… **UX real**: Calidad de respuestas, coherencia
- âœ… **Performance**: Latencia percibida por usuario
- âœ… **Costos reales**: ValidaciÃ³n de budget

### Â¿QuÃ© NO Prueban?

- âŒ Regresiones automÃ¡ticas (muy lento)
- âŒ Edge cases exhaustivos (tomarÃ­a dÃ­as)

### CuÃ¡ndo Usar

```bash
# Antes de deploy a staging
# Seguir: tests/MANUAL_AGENT_TEST_CHECKLIST.md

# Antes de deploy a production
# Seguir: tests/MANUAL_AGENT_TEST_CHECKLIST.md

# DespuÃ©s de bug crÃ­tico en prod
# Reproducir escenario exacto manualmente
```

### Ejemplo

```markdown
### âœ… 3.2. Turno 2: Pregunta de Seguimiento

Ejecutar:
```python
response2 = await agent.query(
    query="Â¿CuÃ¡les de esos tienen problemas?",
    org_id="los_tajibos",
    session_id=session_id,
)
```

**Verificar**:
- [ ] Usa contexto del turno anterior
- [ ] No pide que se repita informaciÃ³n
- [ ] Responde especÃ­ficamente sobre problemas
- [ ] Session ID es el mismo
```

**Costo**: ~$0.02-0.05 USD (2-5 centavos) por checklist completo.

**Valor**: Ãšltima validaciÃ³n antes de producciÃ³n.

---

## ComparaciÃ³n de Estrategias

| Aspecto | Unit Tests | Integration Tests | Manual Tests |
|---------|------------|-------------------|--------------|
| **Tiempo** | ~2 segundos | ~5 minutos | ~20 minutos |
| **Costo** | Gratis | ~$0.005 | ~$0.02-0.05 |
| **Setup** | Ninguno | PostgreSQL + Neo4j + OpenAI | PostgreSQL + Neo4j + OpenAI |
| **AutomatizaciÃ³n** | 100% | 95% | 0% |
| **Confianza** | 60% | 85% | 100% |
| **Cobertura** | LÃ³gica | IntegraciÃ³n | End-to-end |
| **Feedback** | InstantÃ¡neo | 5 min | 20 min |
| **CI/CD** | Siempre | Opcional | No |
| **Regresiones** | Detecta | Detecta | No prÃ¡ctico |

---

## Workflow Recomendado

### Durante Desarrollo (IteraciÃ³n RÃ¡pida)

```bash
# Cada 5-10 minutos mientras codificas
pytest tests/test_agent_session.py -q  # Solo el archivo que estÃ¡s editando
```

**Beneficio**: Feedback inmediato sin esperar.

### Antes de Commit

```bash
# Antes de git commit
pytest tests/test_agent_*.py -v
```

**Beneficio**: Asegura que no rompiste nada.

### Antes de Pull Request

```bash
# Si cambiaste lÃ³gica de conexiones o queries
pytest tests/test_agent_real_integration.py -v -m real_integration
```

**Beneficio**: Valida que tus cambios funcionan con BD reales.

### Antes de Deploy a Staging

```bash
# 1. Unit tests con cobertura
pytest tests/test_agent_*.py --cov=agent --cov-report=html

# 2. Integration tests
pytest tests/test_agent_real_integration.py -v -m real_integration

# 3. Manual checklist
# Ejecutar: tests/MANUAL_AGENT_TEST_CHECKLIST.md

# Solo si los 3 niveles pasan âœ… â†’ Deploy
```

**Beneficio**: MÃ¡xima confianza antes de staging.

### Antes de Deploy a Production

```bash
# Mismos 3 niveles + validaciÃ³n de stakeholders
# + Load testing (si es primera vez en prod)
```

---

## Â¿Por QuÃ© Necesitamos los 3 Niveles?

### Solo Unit Tests (âŒ No Suficiente)

```python
# Test pasa âœ…
mock_pool.execute.return_value = "OK"
response = agent.query("test")
assert response["answer"]

# Pero en producciÃ³n:
# âŒ PostgreSQL no estÃ¡ corriendo
# âŒ Tabla chat_sessions no existe
# âŒ SQL query tiene un typo
# â†’ El agente crashea
```

**Problema**: Mocks ocultan errores de integraciÃ³n.

### Solo Integration Tests (âŒ No PrÃ¡ctico)

```python
# Test requiere:
# - PostgreSQL corriendo â°
# - Neo4j corriendo â°
# - OpenAI API key vÃ¡lida ğŸ’°
# - Datos de prueba cargados â°
# â†’ Toma 5 minutos cada vez

# Durante desarrollo:
# - Editas cÃ³digo
# - Esperas 5 minutos
# - Test falla
# - Editas cÃ³digo
# - Esperas 5 minutos...
# â†’ Muy lento para iterar
```

**Problema**: Demasiado lento para desarrollo iterativo.

### Solo Manual Tests (âŒ No Escalable)

```
# Cada vez que cambias una lÃ­nea:
# 1. Iniciar agente â°
# 2. Ejecutar 8 fases del checklist â°
# 3. Registrar resultados manualmente â°
# â†’ 20 minutos cada vez

# DespuÃ©s de 10 cambios pequeÃ±os:
# â†’ 3+ horas de testing manual
# â†’ Errores por fatiga humana
# â†’ No detecta regresiones automÃ¡ticamente
```

**Problema**: No escala para desarrollo continuo.

### Los 3 Niveles Juntos (âœ… Balance Ã“ptimo)

```
Desarrollo:
â”œâ”€ Unit tests (2 sec) â†’ Feedback instantÃ¡neo
â”œâ”€ Integration tests (5 min) â†’ Valida antes de PR
â””â”€ Manual tests (20 min) â†’ Valida antes de deploy

Resultado:
âœ… Desarrollo rÃ¡pido
âœ… Confianza en integraciones
âœ… ValidaciÃ³n final humana
âœ… Costo razonable (~$0.10/dÃ­a desarrollo)
```

---

## Resumen Ejecutivo

### Â¿CÃ³mo Sabemos que el Agente Funciona?

**3 niveles de validaciÃ³n:**

1. **Unit Tests** â†’ Prueban que la **lÃ³gica** es correcta
2. **Integration Tests** â†’ Prueban que las **conexiones** funcionan
3. **Manual Tests** â†’ Prueban que la **experiencia** es buena

### Â¿CuÃ¡l Usar?

- **Siempre**: Unit tests (rÃ¡pido, automatizable)
- **Cambios en BD**: Integration tests (valida queries reales)
- **Antes de producciÃ³n**: Manual tests (validaciÃ³n final)

### Quick Start

1. **Validar que funciona ahora** (5 min):
   ```bash
   python3 -m agent.example
   ```

2. **Ejecutar unit tests** (2 sec):
   ```bash
   pytest tests/test_agent_*.py -q
   ```

3. **Ejecutar integration tests** (5 min, ~$0.005):
   ```bash
   pytest tests/test_agent_real_integration.py -v -m real_integration
   ```

4. **Manual checklist** (20 min, ~$0.03):
   ```bash
   # Seguir: tests/MANUAL_AGENT_TEST_CHECKLIST.md
   ```

---

## Archivos de Referencia

| Archivo | DescripciÃ³n | CuÃ¡ndo Usar |
|---------|-------------|-------------|
| [agent/QUICK_START.md](../agent/QUICK_START.md) | Ejecutar agente con conexiones reales | Primera vez |
| [tests/TEST_AGENT_GUIDE.md](TEST_AGENT_GUIDE.md) | GuÃ­a completa de testing | Desarrollo |
| [tests/test_agent_*.py](.) | Unit tests con mocks | Siempre |
| [tests/test_agent_real_integration.py](test_agent_real_integration.py) | Integration tests reales | Antes de PR |
| [tests/MANUAL_AGENT_TEST_CHECKLIST.md](MANUAL_AGENT_TEST_CHECKLIST.md) | Checklist de validaciÃ³n manual | Antes de deploy |
| [TASK_10_COMPLETE.md](../TASK_10_COMPLETE.md) | DocumentaciÃ³n de implementaciÃ³n | Referencia |

---

**ConclusiÃ³n**: Los mocks son rÃ¡pidos y confiables para **lÃ³gica**, pero necesitamos integration tests y manual testing para **validar conexiones reales** y **experiencia de usuario**. Los 3 niveles juntos nos dan **mÃ¡xima confianza a mÃ­nimo costo**.
